#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt HTML v√† t√¨m c√°ch tr√≠ch xu·∫•t d·ªØ li·ªáu
"""

import json
import re
import sys
from pathlib import Path
from bs4 import BeautifulSoup

def analyze_html_structure(file_path):
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c HTML ƒë·ªÉ t√¨m d·ªØ li·ªáu
    """
    print(f"üöÄ Ph√¢n t√≠ch HTML t·ª´ file: {file_path}")
    
    try:
        # ƒê·ªçc file RTF
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        print(f"‚úÖ ƒê√£ ƒë·ªçc file: {len(content)} k√Ω t·ª±")
        
        # T√¨m HTML content trong RTF
        html_match = re.search(r'\\f0\\fs28\s*(.*?)(?=\\f0|$)', content, re.DOTALL)
        
        if not html_match:
            print("‚ùå Kh√¥ng t√¨m th·∫•y HTML content trong RTF")
            return None
            
        html_content = html_match.group(1)
        print(f"üìÑ HTML content: {len(html_content)} k√Ω t·ª±")
        
        # Parse HTML v·ªõi BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Ph√¢n t√≠ch c√°c elements ch√≠nh
        print(f"\nüìä Ph√¢n t√≠ch c·∫•u tr√∫c HTML:")
        print(f"   - Title: {soup.title.string if soup.title else 'N/A'}")
        print(f"   - Meta description: {soup.find('meta', attrs={'name': 'description'}).get('content') if soup.find('meta', attrs={'name': 'description'}) else 'N/A'}")
        
        # T√¨m c√°c elements c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu xe
        print(f"\nüîç T√¨m ki·∫øm elements ch·ª©a d·ªØ li·ªáu:")
        
        # 1. T√¨m c√°c div c√≥ class li√™n quan ƒë·∫øn xe
        car_divs = soup.find_all('div', class_=re.compile(r'(car|auto|vehicle|advert|listing)', re.I))
        print(f"   - Divs li√™n quan ƒë·∫øn xe: {len(car_divs)}")
        
        # 2. T√¨m c√°c elements c√≥ data attributes
        data_elements = soup.find_all(attrs={'data-testid': True})
        print(f"   - Elements v·ªõi data-testid: {len(data_elements)}")
        
        # 3. T√¨m c√°c elements c√≥ id li√™n quan
        id_elements = soup.find_all(id=re.compile(r'(car|auto|vehicle|advert|listing)', re.I))
        print(f"   - Elements v·ªõi id li√™n quan: {len(id_elements)}")
        
        # 4. T√¨m c√°c script tags
        script_tags = soup.find_all('script')
        print(f"   - Script tags: {len(script_tags)}")
        
        # 5. T√¨m c√°c elements c√≥ text ch·ª©a t·ª´ kh√≥a xe
        car_keywords = ['BMW', 'Audi', 'Mercedes', 'Volkswagen', 'Opel', 'Ford', '‚Ç¨', 'km', 'Jahr']
        for keyword in car_keywords:
            elements = soup.find_all(text=re.compile(keyword, re.I))
            if elements:
                print(f"   - Elements ch·ª©a '{keyword}': {len(elements)}")
        
        return soup
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ph√¢n t√≠ch HTML: {e}")
        return None

def extract_data_from_elements(soup):
    """
    Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ c√°c HTML elements
    """
    print(f"\nüîç Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ HTML elements:")
    
    extracted_data = []
    
    # 1. T√¨m c√°c elements c√≥ th·ªÉ ch·ª©a th√¥ng tin xe
    potential_car_elements = []
    
    # T√¨m c√°c div c√≥ class ch·ª©a t·ª´ kh√≥a li√™n quan
    for div in soup.find_all('div'):
        class_name = ' '.join(div.get('class', []))
        if any(keyword in class_name.lower() for keyword in ['car', 'auto', 'vehicle', 'advert', 'listing', 'item']):
            potential_car_elements.append(div)
    
    print(f"   - T√¨m th·∫•y {len(potential_car_elements)} potential car elements")
    
    # 2. Tr√≠ch xu·∫•t th√¥ng tin t·ª´ m·ªói element
    for i, element in enumerate(potential_car_elements[:10]):  # Ch·ªâ l·∫•y 10 ƒë·∫ßu ti√™n
        print(f"\nüöó Element {i+1}:")
        
        car_data = {}
        
        # Tr√≠ch xu·∫•t text content
        text_content = element.get_text(strip=True)
        if text_content:
            car_data['text'] = text_content[:200] + "..." if len(text_content) > 200 else text_content
        
        # Tr√≠ch xu·∫•t attributes
        car_data['attributes'] = dict(element.attrs)
        
        # T√¨m c√°c elements con c√≥ th·ªÉ ch·ª©a th√¥ng tin c·ª• th·ªÉ
        title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        if title_elem:
            car_data['title'] = title_elem.get_text(strip=True)
        
        price_elem = element.find(text=re.compile(r'‚Ç¨\s*\d+'))
        if price_elem:
            car_data['price'] = price_elem.strip()
        
        # T√¨m links
        links = element.find_all('a', href=True)
        if links:
            car_data['links'] = [link['href'] for link in links]
        
        # T√¨m images
        images = element.find_all('img', src=True)
        if images:
            car_data['images'] = [img['src'] for img in images]
        
        extracted_data.append(car_data)
        
        # In th√¥ng tin c∆° b·∫£n
        print(f"   - Title: {car_data.get('title', 'N/A')}")
        print(f"   - Price: {car_data.get('price', 'N/A')}")
        print(f"   - Text: {car_data.get('text', 'N/A')[:100]}...")
    
    return extracted_data

def search_for_json_patterns(html_content):
    """
    T√¨m ki·∫øm c√°c pattern JSON c√≥ th·ªÉ c√≥ trong HTML
    """
    print(f"\nüîç T√¨m ki·∫øm JSON patterns trong HTML:")
    
    # C√°c pattern c√≥ th·ªÉ ch·ª©a JSON
    patterns = [
        r'window\.__NEXT_DATA__\s*=\s*(\{.*?\});',
        r'window\.__INITIAL_STATE__\s*=\s*(\{.*?\});',
        r'window\.__DATA__\s*=\s*(\{.*?\});',
        r'var\s+data\s*=\s*(\{.*?\});',
        r'const\s+data\s*=\s*(\{.*?\});',
        r'let\s+data\s*=\s*(\{.*?\});',
        r'data:\s*(\{.*?\})',
        r'"data":\s*(\{.*?\})',
        r'"adverts":\s*(\[.*?\])',
        r'"ads":\s*(\[.*?\])',
        r'"listings":\s*(\[.*?\])',
        r'"results":\s*(\[.*?\])',
    ]
    
    found_patterns = []
    
    for i, pattern in enumerate(patterns):
        matches = re.findall(pattern, html_content, re.DOTALL)
        if matches:
            print(f"   Pattern {i+1}: T√¨m th·∫•y {len(matches)} matches")
            for j, match in enumerate(matches[:3]):  # Ch·ªâ l·∫•y 3 ƒë·∫ßu ti√™n
                print(f"     Match {j+1}: {len(match)} k√Ω t·ª±")
                print(f"     B·∫Øt ƒë·∫ßu: {match[:100]}...")
                
                # Th·ª≠ parse JSON
                try:
                    json_obj = json.loads(match)
                    print(f"     ‚úÖ JSON h·ª£p l·ªá!")
                    print(f"     Keys: {list(json_obj.keys())[:5]}...")
                    found_patterns.append(json_obj)
                except json.JSONDecodeError:
                    print(f"     ‚ùå Kh√¥ng ph·∫£i JSON h·ª£p l·ªá")
        else:
            print(f"   Pattern {i+1}: Kh√¥ng t√¨m th·∫•y")
    
    return found_patterns

def main():
    if len(sys.argv) != 2:
        print("Usage: python analyze_html.py <path_to_rtf_file>")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    if not Path(file_path).exists():
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}")
        sys.exit(1)
    
    # Ph√¢n t√≠ch c·∫•u tr√∫c HTML
    soup = analyze_html_structure(file_path)
    if not soup:
        sys.exit(1)
    
    # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ elements
    extracted_data = extract_data_from_elements(soup)
    
    # T√¨m ki·∫øm JSON patterns
    html_content = str(soup)
    json_patterns = search_for_json_patterns(html_content)
    
    # L∆∞u k·∫øt qu·∫£
    results = {
        'extracted_data': extracted_data,
        'json_patterns': json_patterns,
        'summary': {
            'total_elements': len(extracted_data),
            'json_found': len(json_patterns)
        }
    }
    
    output_file = "html_analysis_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o file: {output_file}")
    
    # In t·ªïng k·∫øt
    print(f"\nüìä T·ªïng k·∫øt ph√¢n t√≠ch:")
    print(f"   - Elements ƒë∆∞·ª£c tr√≠ch xu·∫•t: {len(extracted_data)}")
    print(f"   - JSON patterns t√¨m th·∫•y: {len(json_patterns)}")
    print(f"   - File output: {output_file}")
    
    if extracted_data:
        print(f"\nüéØ M·ªôt s·ªë d·ªØ li·ªáu ƒë∆∞·ª£c tr√≠ch xu·∫•t:")
        for i, data in enumerate(extracted_data[:3]):
            print(f"   {i+1}. {data.get('title', 'N/A')} - {data.get('price', 'N/A')}")

if __name__ == "__main__":
    main()
