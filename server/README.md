# Willhaben.at Realtime Crawler Backend

Backend Python realtime crawl d·ªØ li·ªáu t·ª´ trang web Willhaben.at s·ª≠ d·ª•ng FastAPI + aiohttp + WebSocket ƒë·ªÉ ph√°t hi·ªán v√† g·ª≠i tin m·ªõi realtime.

## üöÄ T√≠nh nƒÉng

- ‚úÖ **Realtime Crawling**: Crawl d·ªØ li·ªáu t·ª´ Willhaben.at m·ªói 3 gi√¢y
- ‚úÖ **Diff Detection**: Ph√°t hi·ªán tin m·ªõi b·∫±ng c√°ch l∆∞u cache ID c√°c tin ƒë√£ th·∫•y
- ‚úÖ **WebSocket Realtime**: G·ª≠i tin m·ªõi realtime qua WebSocket ƒë·∫øn client
- ‚úÖ **Async Workers**: Nhi·ªÅu worker async ƒë·ªÉ crawl song song
- ‚úÖ **Anti-Detection**: Random delay, user-agent rotation, proxy rotation, auto-fetch free proxy ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
- ‚úÖ **Health Check**: Endpoint `/health` tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng tin ƒë√£ th·∫•y
- ‚úÖ **Test Page**: Trang `/test` demo WebSocket trong tr√¨nh duy·ªát
- ‚úÖ **Logging**: Logging chi ti·∫øt v√† c√≥ th·ªÉ m·ªü r·ªông
- ‚úÖ **Scalable**: S·∫µn s√†ng t√≠ch h·ª£p Redis, Firebase, Database

## üìã Y√™u c·∫ßu h·ªá th·ªëng

- Python 3.8+
- pip ho·∫∑c conda

## üõ†Ô∏è C√†i ƒë·∫∑t

### 1. Clone ho·∫∑c t·∫£i project

```bash
# N·∫øu b·∫°n c√≥ git
git clone <repository-url>
cd Willhaben

# Ho·∫∑c t·∫£i file realtime_backend.py v√† requirements.txt
```

### 2. C√†i ƒë·∫∑t dependencies

```bash
# C√°ch ƒë∆°n gi·∫£n nh·∫•t (s·ª≠ d·ª•ng html.parser)
pip install -r requirements.txt

# N·∫øu g·∫∑p l·ªói lxml, xem INSTALL.md ƒë·ªÉ bi·∫øt c√°ch kh·∫Øc ph·ª•c
```

### 3. Ch·∫°y ·ª©ng d·ª•ng

```bash
# C√°ch 1: Ch·∫°y tr·ª±c ti·∫øp
python realtime_backend.py

# C√°ch 2: S·ª≠ d·ª•ng uvicorn (khuy·∫øn ngh·ªã)
uvicorn realtime_backend:app --reload --host 0.0.0.0 --port 8000
```

## üåê Endpoints

### HTTP Endpoints

- **GET /** - Trang ch·ªß v·ªõi th√¥ng tin API
- **GET /health** - Health check v√† th·ªëng k√™
- **GET /test** - Trang test WebSocket trong tr√¨nh duy·ªát

### WebSocket Endpoint

- **WS /ws** - WebSocket cho realtime updates

## üì± T√≠ch h·ª£p Android App

### K·∫øt n·ªëi WebSocket

```kotlin
// Android Kotlin example
val wsUrl = "ws://your-server-ip:8000/ws"
val webSocket = OkHttpClient().newWebSocket(
    Request.Builder().url(wsUrl).build(),
    object : WebSocketListener() {
        override fun onMessage(webSocket: WebSocket, text: String) {
            try {
                val data = JSONObject(text)
                if (data.getString("type") == "new_listing") {
                    val listing = data.getJSONObject("data")
                    // X·ª≠ l√Ω tin m·ªõi
                    handleNewListing(listing)
                }
            } catch (e: Exception) {
                e.printStackTrace()
            }
        }
    }
)
```

### C·∫•u tr√∫c d·ªØ li·ªáu tin m·ªõi

```json
{
  "type": "new_listing",
  "data": {
    "id": "123456789",
    "title": "BMW 320d xDrive",
    "price": "‚Ç¨ 25.900",
    "year": "2019",
    "km": "45.000 km",
    "link": "https://www.willhaben.at/iad/gebrauchtwagen/auto/...",
    "image_url": "https://...",
    "crawled_at": "2024-01-01T12:00:00",
    "source": "willhaben.at"
  },
  "timestamp": "2024-01-01T12:00:00"
}
```

## üîß C·∫•u h√¨nh

### Thay ƒë·ªïi c·∫•u h√¨nh trong file `realtime_backend.py`

```python
class Config:
    # Crawler settings
    BASE_URL = "https://www.willhaben.at/iad/gebrauchtwagen/auto/gebrauchtwagenboerse"
    CRAWL_INTERVAL = 3.0  # Gi√¢y gi·ªØa c√°c l·∫ßn crawl
    MAX_WORKERS = 5  # S·ªë worker async song song
    REQUEST_TIMEOUT = 10  # Timeout cho HTTP requests
    
    # Anti-detection settings
    MIN_DELAY = 0.5  # Delay t·ªëi thi·ªÉu gi·ªØa requests
    MAX_DELAY = 2.0  # Delay t·ªëi ƒëa gi·ªØa requests
    USER_AGENT_ROTATION = True
    
    # Proxy settings
    USE_PROXY_ROTATION = True  # B·∫≠t proxy rotation
    AUTO_FETCH_FREE_PROXY = True  # T·ª± ƒë·ªông l·∫•y proxy free
    PROXY_LIST = [
        # Proxy th·ªß c√¥ng (t√πy ch·ªçn)
        # "http://proxy1:port",
        # "http://proxy2:port",
        # "http://username:password@proxy3:port"
    ]
    PROXY_TIMEOUT = 15  # Timeout cho proxy
    MAX_FREE_PROXIES = 50  # S·ªë l∆∞·ª£ng proxy free t·ªëi ƒëa
    PROXY_FETCH_INTERVAL = 300  # Th·ªùi gian fetch proxy m·ªõi (gi√¢y)
```

## üìä Monitoring

### Health Check Response

```json
{
  "status": "healthy",
  "crawler_running": true,
  "total_seen_items": 150,
  "total_crawled": 150,
  "new_items_found": 5,
  "uptime_seconds": 3600,
  "last_crawl_time": "2024-01-01T12:00:00",
  "websocket_connections": 2
}
```

### Logs

·ª®ng d·ª•ng s·∫Ω t·∫°o file log `willhaben_crawler.log` v·ªõi th√¥ng tin chi ti·∫øt:

```
2024-01-01 12:00:00 - WillhabenCrawler - INFO - Starting crawl cycle...
2024-01-01 12:00:01 - WillhabenCrawler - INFO - Found 20 potential car listings
2024-01-01 12:00:02 - WillhabenCrawler - INFO - New listing found: BMW 320d - ‚Ç¨ 25.900
2024-01-01 12:00:03 - WebSocketManager - INFO - Broadcasted message to 2 connections
```

## üöÄ M·ªü r·ªông

### Th√™m Redis Cache

```python
import redis
import json

class RedisCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    async def store_listing(self, listing_id: str, data: dict):
        self.redis_client.setex(f"listing:{listing_id}", 3600, json.dumps(data))
    
    async def get_listing(self, listing_id: str):
        data = self.redis_client.get(f"listing:{listing_id}")
        return json.loads(data) if data else None
```

### Th√™m Database

```python
from sqlalchemy import create_engine, Column, String, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class CarListing(Base):
    __tablename__ = "car_listings"
    
    id = Column(String, primary_key=True)
    title = Column(String)
    price = Column(String)
    year = Column(String)
    km = Column(String)
    link = Column(String)
    image_url = Column(String)
    crawled_at = Column(DateTime)
    source = Column(String)
```

### Proxy Rotation + Auto-Fetch Free Proxy (ƒê√£ t√≠ch h·ª£p s·∫µn)

Backend ƒë√£ t√≠ch h·ª£p ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng proxy:

```python
# C·∫•u h√¨nh proxy trong Config
USE_PROXY_ROTATION = True
AUTO_FETCH_FREE_PROXY = True  # T·ª± ƒë·ªông l·∫•y proxy free
PROXY_LIST = [
    # Proxy th·ªß c√¥ng (t√πy ch·ªçn)
    "http://proxy1:port",
    "http://proxy2:port", 
    "http://username:password@proxy3:port"
]

# Endpoints qu·∫£n l√Ω proxy
GET /proxy/stats      # Xem th·ªëng k√™ proxy
GET /proxy/list       # Xem danh s√°ch proxy
POST /proxy/fetch     # Fetch proxy free th·ªß c√¥ng
POST /proxy/reset     # Reset proxy b·ªã l·ªói
```

**T√≠nh nƒÉng t·ª± ƒë·ªông:**
- ‚úÖ T·ª± ƒë·ªông fetch proxy free t·ª´ 4+ ngu·ªìn
- ‚úÖ Test proxy v√† ch·ªâ gi·ªØ l·∫°i proxy ho·∫°t ƒë·ªông
- ‚úÖ Fetch proxy m·ªõi m·ªói 5 ph√∫t
- ‚úÖ T·ª± ƒë·ªông failover khi proxy b·ªã l·ªói

Xem chi ti·∫øt trong file `FREE_PROXY_GUIDE.md` v√† `PROXY_CONFIG.md`

## üêõ Troubleshooting

### L·ªói th∆∞·ªùng g·∫∑p

1. **L·ªói lxml build**: Xem file `INSTALL.md` ƒë·ªÉ bi·∫øt c√°ch kh·∫Øc ph·ª•c chi ti·∫øt
   ```bash
   # Gi·∫£i ph√°p nhanh: S·ª≠ d·ª•ng html.parser
   pip install -r requirements.txt
   ```

2. **Import Error**: ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t t·∫•t c·∫£ dependencies
   ```bash
   pip install -r requirements.txt
   ```

3. **WebSocket Connection Failed**: Ki·ªÉm tra firewall v√† port 8000
   ```bash
   # Ki·ªÉm tra port c√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng
   lsof -i :8000
   ```

4. **Crawling kh√¥ng ho·∫°t ƒë·ªông**: Ki·ªÉm tra network v√† URL
   ```bash
   curl -I https://www.willhaben.at/iad/gebrauchtwagen/auto/gebrauchtwagenboerse
   ```

### Debug Mode

ƒê·ªÉ b·∫≠t debug mode, thay ƒë·ªïi trong `realtime_backend.py`:

```python
class Config:
    LOG_LEVEL = logging.DEBUG  # Thay ƒë·ªïi t·ª´ INFO th√†nh DEBUG
```

## üìù License

MIT License - Xem file LICENSE ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.

## ü§ù Contributing

1. Fork project
2. T·∫°o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. M·ªü Pull Request

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, h√£y t·∫°o issue tr√™n GitHub ho·∫∑c li√™n h·ªá qua email.

---

**L∆∞u √Ω**: ƒê·∫£m b·∫£o tu√¢n th·ªß Terms of Service c·ªßa Willhaben.at khi s·ª≠ d·ª•ng crawler n√†y.
